# ğŸ¤– **LLMOps Multi-AI Agent â€” Project Overview**

This repository presents a complete **LLMOps workflow** for a **Multi-AI Agent chatbot**, powered by **Groq LLMs**, optional **Tavily web search**, and a clean **FastAPI + Streamlit** architecture.
It extends beyond local experimentation into a full **CI/CD pipeline** with:

* **Jenkins (Docker-in-Docker)**
* **SonarQube code-quality analysis**
* **AWS Elastic Container Registry (ECR) for image storage**
* **AWS ECS Fargate** for serverless container deployment

From a single UI, users can select roles (e.g. technical expert, journalist), choose LLM models, toggle web search, and interact with the agent â€” all backed by a production-style MLOps stack.

<p align="center">
  <img src="img/streamlit/streamlit_app.gif" alt="Multi-AI Agent Streamlit Demo" width="100%">
</p>

## ğŸ§© **Grouped Stages**

|     #     | Stage                                    | Description                                                                                                                                                                                                                                                       |
| :-------: | :--------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|   **00**  | **Project Setup**                        | Established the base VS Code structure, virtual environment, `.env` secrets, logging and exception utilities, and configuration settings for API keys and model selection.                                                                                        |
|   **01**  | **WSL & Ubuntu Environment**             | Installed **WSL** and **Ubuntu** on Windows, configured Docker Engine inside Ubuntu, and prepared a Linux-based development environment for containers and CI tooling.                                                                                            |
| **02â€“04** | **Core Multi-AI Agent Logic**            | Implemented the Groq-backed LangGraph agent (`ai_agent.py`), a **FastAPI** backend API for the `/chat` endpoint, and an initial **Streamlit** UI for sending queries, selecting models, and toggling web search.                                                  |
|   **05**  | **Unified Application Launcher**         | Added a single `main.py` entry point to start both the Uvicorn backend and the Streamlit frontend together, enabling end-to-end app execution with one command.                                                                                                   |
|   **06**  | **Jenkins Setup (Docker-in-Docker)**     | Built a custom **Jenkins DinD** image, installed Docker and Python inside the container, and configured Jenkins to run in WSL with full Docker access for building images.                                                                                        |
|   **07**  | **GitHub Integration with Jenkins**      | Generated a GitHub personal access token, added it as credentials in Jenkins, created a Pipeline job, and wired a **Jenkinsfile** so Jenkins can clone the repository and run pipeline stages from source control.                                                |
|   **08**  | **SonarQube Code Quality Integration**   | Deployed **SonarQube** via Docker, installed Sonar plugins in Jenkins, configured a scanner and server connection, added a SonarQube analysis stage to the Jenkinsfile, and inspected code quality reports in the SonarQube UI.                                   |
|   **09**  | **AWS ECR â€“ Build & Push Docker Images** | Installed AWS CLI in Jenkins, created an IAM user and ECR repository, added AWS credentials to Jenkins, and implemented a pipeline stage that builds the project Docker image and pushes it to **Amazon ECR**.                                                    |
|   **10**  | **AWS ECS Fargate Deployment**           | Created an ECS Fargate cluster and task definition pointing to the ECR image, configured ports and environment variables, updated security groups, and added a final Jenkins stage to trigger **ECS service updates**, exposing the Streamlit UI via a public IP. |

## ğŸ—‚ï¸ **Project Structure**

```text
LLMOPS-MULTI-AI-AGENT/
â”œâ”€â”€ Dockerfile                         # ğŸ³ Builds the Multi-AI Agent app image (backend + frontend)
â”œâ”€â”€ Jenkinsfile                        # âš™ï¸ Jenkins CI/CD pipeline (SonarQube, ECR build/push, ECS deploy)
â”œâ”€â”€ custom_jenkins/                    # ğŸ§± Custom Jenkins DinD image definition
â”‚   â””â”€â”€ Dockerfile                     # Docker-in-Docker Jenkins image with Docker + AWS CLI installed
â”œâ”€â”€ img/                               # ğŸ“¸ Screenshots and GIFs for documentation
â”œâ”€â”€ .venv/                             # Local Python virtual environment (managed via uv / venv)
â”œâ”€â”€ .env                               # ğŸ” Environment variables (GROQ_API_KEY, TAVILY_API_KEY, etc.)
â”œâ”€â”€ .gitignore                         # Ignore rules for venv, logs, artefacts, and OS files
â”œâ”€â”€ .python-version                    # Python version pin for consistent environments
â”œâ”€â”€ llmops_multi_ai_agent.egg-info/    # ğŸ“¦ Auto-generated packaging metadata
â”œâ”€â”€ pyproject.toml                     # ğŸ§© Project metadata, dependencies, and build configuration
â”œâ”€â”€ README.md                          # ğŸ“– Main project documentation (you are here)
â”œâ”€â”€ requirements.txt                   # ğŸ“¦ Python dependencies (FastAPI, Streamlit, LangChain, Groq, etc.)
â”œâ”€â”€ setup.py                           # ğŸ”§ Editable install configuration for packaging
â”œâ”€â”€ uv.lock                            # ğŸ”’ Exact dependency lockfile generated by uv
â”‚
â””â”€â”€ app/                               # ğŸ§  Application package (backend, frontend, core agent)
    â”œâ”€â”€ main.py                        # ğŸš€ Unified launcher that starts backend (Uvicorn) + frontend (Streamlit)
    â”œâ”€â”€ backend/                       # ğŸŒ Backend API layer (FastAPI)
    â”‚   â””â”€â”€ api.py                     # `/chat` endpoint: validates requests, calls AI agent, handles errors
    â”œâ”€â”€ common/                        # ğŸªµ Shared utilities for reliability and observability
    â”‚   â”œâ”€â”€ custom_exception.py        # Rich `CustomException` class with file/line context for errors
    â”‚   â””â”€â”€ logger.py                  # Centralised logging setup for console and structured logs
    â”œâ”€â”€ config/                        # âš™ï¸ Configuration and environment management
    â”‚   â””â”€â”€ settings.py                # Loads API keys, allowed model names, and global settings from `.env`
    â”œâ”€â”€ core/                          # ğŸ§  Core reasoning logic
    â”‚   â””â”€â”€ ai_agent.py                # LangGraph/Groq-based ReAct-style agent with optional Tavily search
    â””â”€â”€ frontend/                      # ğŸ¨ User-facing UI layer
        â””â”€â”€ ui.py                      # Streamlit web UI: roles, model selection, web search toggle, chat interface
```

## ğŸš€ **Summary**

The **LLMOps Multi-AI Agent** project shows how to take a modern LLM-powered application from **local prototype** to a **production-style cloud deployment pipeline**.

It combines:

* A Groq-backed, LangGraph-powered multi-AI agent with optional Tavily web search
* A clean split between **FastAPI backend** and **Streamlit frontend**
* Robust logging and exception handling for better debuggability
* **Jenkins (Docker-in-Docker)** for CI orchestration
* **SonarQube** for continuous code-quality analysis
* **AWS ECR** for container image storage
* **AWS ECS Fargate** for serverless, scalable deployment behind a public IP

Together, these stages form a complete **LLMOps / DevOps story**:
from writing a prompt in the Streamlit UI, through CI quality gates and container builds, all the way to running the chatbot as a managed service in the cloud.
